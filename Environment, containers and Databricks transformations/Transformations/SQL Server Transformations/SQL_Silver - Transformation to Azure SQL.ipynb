{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a011120f-7adf-40b3-bbef-03ebee4f866b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "sql_server_name = \"xxxxxxx-beverage-sql-server.database.windows.net\"\n",
    "sql_database = \"xxxxxxx-beverage-sql\" \n",
    "sql_user = \"xxxxxxx_teste\"\n",
    "sql_password = \"xxxxxxxxxxxx\"\n",
    "\n",
    "# JDBC connection\n",
    "jdbc_url = f\"jdbc:sqlserver://{sql_server_name}:1433;database={sql_database};encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.database.windows.net;loginTimeout=30;\"\n",
    "\n",
    "connection_properties = {\n",
    "    \"user\": sql_user,\n",
    "    \"password\": sql_password, \n",
    "    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d67bd45-ccd1-4f5d-8d23-ada0c532a1ea",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Read from files to df with Schema"
    }
   },
   "outputs": [],
   "source": [
    "import chardet\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql.functions import col, to_date, regexp_replace, trim, upper, expr, row_number, abs\n",
    "\n",
    "# Detect file encoding\n",
    "def detect_encoding_spark(spark, path, sample_size=1024):\n",
    "    try:\n",
    "        binary_rdd = spark.sparkContext.binaryFiles(path).map(lambda x: x[1])\n",
    "        sample_bytes = binary_rdd.take(1)[0][:sample_size]\n",
    "        result = chardet.detect(sample_bytes)\n",
    "        return result['encoding']\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao detectar encoding: {str(e)}\")\n",
    "        return \"UTF-8\"\n",
    "\n",
    "# Simplified Schema importing from CSV as String\n",
    "simple_schema = StructType([StructField(c.upper(), StringType(), True) for c in [\n",
    "    \"DATE\", \"CE_BRAND_FLVR\", \"BRAND_NM\", \"BTLR_ORG_LVL_C_DESC\",\n",
    "    \"CHNL_GROUP\", \"TRADE_CHNL_DESC\", \"PKG_CAT\", \"PKG_CAT_DESC\",\n",
    "    \"TSR_PCKG_NM\", \"$ VOLUME\", \"YEAR\", \"MONTH\", \"PERIOD\"\n",
    "]])\n",
    "\n",
    "# Read from CSV with previous detected encoding\n",
    "def read_csv_simple(spark, path, delimiter=\"\\t\"):\n",
    "    encoding = detect_encoding_spark(spark, path)\n",
    "\n",
    "    df = spark.read.format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"encoding\", encoding if encoding != 'UTF-8-SIG' else 'UTF-8') \\\n",
    "        .option(\"delimiter\", delimiter) \\\n",
    "        .schema(simple_schema) \\\n",
    "        .load(path)\n",
    "    df_upper = df.toDF(*[col_name.upper() for col_name in df.columns])\n",
    "    return df_upper\n",
    "\n",
    "# Start spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Read beverage_sales, correct the date format and TSR_PCKG_NM standardization\n",
    "beverage_sales_df = read_csv_simple(\n",
    "    spark,\n",
    "    \"/mnt/bronze/beverage_sales/xxx_bus_case1_beverage_sales_20210726.csv\",\n",
    "    \"\\t\"\n",
    ").select(\n",
    "    to_date(\n",
    "        regexp_replace(col(\"DATE\"), r\"^(\\d{1})/\", r\"0$1/\"),\n",
    "        \"MM/dd/yyyy\"\n",
    "    ).alias(\"DATE\"),\n",
    "\n",
    "    col(\"CE_BRAND_FLVR\").cast(\"integer\"),\n",
    "    col(\"BRAND_NM\"),\n",
    "    col(\"BTLR_ORG_LVL_C_DESC\"),\n",
    "    col(\"CHNL_GROUP\"),\n",
    "    col(\"TRADE_CHNL_DESC\"),\n",
    "    col(\"PKG_CAT\"),\n",
    "    col(\"PKG_CAT_DESC\"),\n",
    "    upper(trim(regexp_replace(col(\"TSR_PCKG_NM\"), r\"( \\*| S|\\*|S)$\", \"\"))).alias(\"TSR_PCKG_NM\"), # Clean TSR_PCKG_NM removing sufixes\n",
    "    abs(col(\"$ VOLUME\").cast(\"double\")).alias(\"$ VOLUME\"), # absolute numbers, negative to positive\n",
    "    col(\"YEAR\").cast(\"integer\"),\n",
    "    col(\"MONTH\").cast(\"integer\"),\n",
    "    col(\"PERIOD\").cast(\"integer\")\n",
    ")\n",
    "\n",
    "# Read channel_features\n",
    "channel_features_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"/mnt/bronze/beverage_channel_features/xxx_bus_case1_beverage_channel_group_20210726.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9b2f75a-6093-4226-a151-e3e2b6df0b57",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Merge dataframes"
    }
   },
   "outputs": [],
   "source": [
    "# Merge\n",
    "merged_df = beverage_sales_df.join(\n",
    "    channel_features_df,\n",
    "    on=\"TRADE_CHNL_DESC\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Show Schema\n",
    "# merged_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aad1d091-1e61-4d25-9ac0-7aad835ea8aa",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create dim_flavor"
    }
   },
   "outputs": [],
   "source": [
    "# dim_flavor\n",
    "dim_flavor = merged_df.select(\n",
    "    col(\"CE_BRAND_FLVR\").alias(\"flavor_id\"),\n",
    "    col(\"BRAND_NM\").alias(\"flavor_description\")\n",
    ").distinct()\n",
    "dim_flavor.write.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"sql_dim_flavor\",\n",
    "    mode=\"overwrite\",\n",
    "    properties=connection_properties\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ddd1aa8-06ed-4d5f-925a-2518e9d1a8d3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create dim_package"
    }
   },
   "outputs": [],
   "source": [
    "# dim_package\n",
    "dim_package = merged_df.select(\n",
    "    col(\"PKG_CAT\").alias(\"package_category_code\"),\n",
    "    col(\"PKG_CAT_DESC\").alias(\"package_category_description\"),\n",
    "    col(\"TSR_PCKG_NM\").alias(\"package_name\")\n",
    ").distinct()\n",
    "dim_package.write.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"sql_dim_package\",\n",
    "    mode=\"overwrite\",\n",
    "    properties=connection_properties\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "364f5558-46f3-4fe1-9097-3f2447b93040",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create dim_channel_group"
    }
   },
   "outputs": [],
   "source": [
    "# dim_channel_group\n",
    "dim_channel_group = merged_df.select(\n",
    "    col(\"TRADE_CHNL_DESC\").alias(\"channel_id\"),\n",
    "    col(\"CHNL_GROUP\").alias(\"channel_group\"),\n",
    "    col(\"TRADE_GROUP_DESC\").alias(\"trade_group_desc\"),\n",
    "    col(\"TRADE_TYPE_DESC\").alias(\"trade_type_desc\")\n",
    ").distinct()\n",
    "dim_channel_group.write.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"sql_dim_channel_group\",\n",
    "    mode=\"overwrite\",\n",
    "    properties=connection_properties\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d55d78a-6149-45c7-b68c-4b986f137ee8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create dim_month"
    }
   },
   "outputs": [],
   "source": [
    "# dim_month\n",
    "dim_month = merged_df.select(\n",
    "    col(\"MONTH\").alias(\"month_id\")\n",
    ").distinct().withColumn(\n",
    "    \"month_name\",\n",
    "    expr(\"\"\"\n",
    "        CASE month_id\n",
    "            WHEN 1 THEN 'January'\n",
    "            WHEN 2 THEN 'February'\n",
    "            WHEN 3 THEN 'March'\n",
    "            WHEN 4 THEN 'April'\n",
    "            WHEN 5 THEN 'May'\n",
    "            WHEN 6 THEN 'June'\n",
    "            WHEN 7 THEN 'July'\n",
    "            WHEN 8 THEN 'August'\n",
    "            WHEN 9 THEN 'September'\n",
    "            WHEN 10 THEN 'October'\n",
    "            WHEN 11 THEN 'November'\n",
    "            WHEN 12 THEN 'December'\n",
    "        END\n",
    "    \"\"\"))\n",
    "dim_month.write.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"sql_dim_month\",\n",
    "    mode=\"overwrite\",\n",
    "    properties=connection_properties\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9a31f20-30d7-4f1b-9948-1dd193b2746d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create fact_sale"
    }
   },
   "outputs": [],
   "source": [
    "# fact_sale\n",
    "windowSpec = Window.orderBy(\n",
    "    col(\"selling_date\"),\n",
    "    col(\"flavor_id\"),\n",
    "    col(\"region_desc\"),\n",
    "    col(\"channel_id\"))\n",
    "# fact_table and reorganize columns\n",
    "fact_sale = merged_df.select(\n",
    "    col(\"DATE\").alias(\"selling_date\"),\n",
    "    col(\"YEAR\").alias(\"year\"),\n",
    "    col(\"MONTH\").alias(\"month_id\"),\n",
    "    col(\"PERIOD\").alias(\"week\"),\n",
    "    col(\"CE_BRAND_FLVR\").alias(\"flavor_id\"),\n",
    "    col(\"BTLR_ORG_LVL_C_DESC\").alias(\"region_desc\"),\n",
    "    col(\"PKG_CAT\").alias(\"package_category_code\"),\n",
    "    col(\"$ VOLUME\").alias(\"sales_volume\"),\n",
    "    col(\"TRADE_CHNL_DESC\").alias(\"channel_id\")\n",
    ").withColumn(\"sale_id\", row_number().over(windowSpec)) \\\n",
    " .select(\"sale_id\", \"selling_date\", \"year\", \"month_id\", \"week\",\n",
    "         \"flavor_id\", \"region_desc\", \"package_category_code\",\n",
    "         \"sales_volume\", \"channel_id\")\n",
    "fact_sale.write.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"sql_fact_sale\",\n",
    "    mode=\"overwrite\",\n",
    "    properties=connection_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fd7e6f5-9ff9-440a-ae66-904aa4d9ff05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#display(fact_sale.limit(5))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) SQL_Silver - Transformation to Azure SQL",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}